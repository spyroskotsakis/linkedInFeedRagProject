# open_clip â€” ðŸ¦œðŸ”— LangChain  documentation

**URL:** https://python.langchain.com/api_reference/experimental/open_clip.html
**Word Count:** 31
**Links Count:** 94
**Scraped:** 2025-07-21 08:24:51
**Status:** completed

---

# `open_clip`\#

**OpenCLIP Embeddings** model.

OpenCLIP is a multimodal model that can encode text and images into a shared space.

See this paper for more details: <https://arxiv.org/abs/2103.00020> and \[this repository\]\([mlfoundations/open\_clip](https://github.com/mlfoundations/open_clip)\) for details.

**Classes**

[`open_clip.open_clip.OpenCLIPEmbeddings`](https://python.langchain.com/api_reference/experimental/open_clip/langchain_experimental.open_clip.open_clip.OpenCLIPEmbeddings.html#langchain_experimental.open_clip.open_clip.OpenCLIPEmbeddings "langchain_experimental.open_clip.open_clip.OpenCLIPEmbeddings") | OpenCLIP Embeddings model.   ---|---