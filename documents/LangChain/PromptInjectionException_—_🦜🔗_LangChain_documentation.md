# PromptInjectionException â€” ðŸ¦œðŸ”— LangChain  documentation

**URL:** https://python.langchain.com/api_reference/experimental/prompt_injection_identifier/langchain_experimental.prompt_injection_identifier.hugging_face_identifier.PromptInjectionException.html
**Word Count:** 14
**Links Count:** 96
**Scraped:** 2025-07-21 08:23:33
**Status:** completed

---

# PromptInjectionException\#

_class _langchain\_experimental.prompt\_injection\_identifier.hugging\_face\_identifier.PromptInjectionException\(

    _message : str = 'Prompt injection attack detected'_,     _score : float = 1.0_, \)[\[source\]](https://python.langchain.com/api_reference/_modules/langchain_experimental/prompt_injection_identifier/hugging_face_identifier.html#PromptInjectionException)\#     

Exception raised when prompt injection attack is detected.

Parameters:     

  * **message** \(_str_\)

  * **score** \(_float_\)

__On this page