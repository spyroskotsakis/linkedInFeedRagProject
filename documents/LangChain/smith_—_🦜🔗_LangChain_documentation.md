# smith â€” ðŸ¦œðŸ”— LangChain  documentation

**URL:** https://python.langchain.com/api_reference/langchain/smith.html
**Word Count:** 282
**Links Count:** 120
**Scraped:** 2025-07-21 07:50:36
**Status:** completed

---

# `smith`\#

**LangSmith** utilities.

This module provides utilities for connecting to [LangSmith](https://smith.langchain.com/). For more information on LangSmith, see the [LangSmith documentation](https://docs.smith.langchain.com/).

**Evaluation**

LangSmith helps you evaluate Chains and other language model application components using a number of LangChain evaluators. An example of this is shown below, assuming youâ€™ve created a LangSmith dataset called `<my_dataset_name>`:               from langsmith import Client     from langchain_community.chat_models import ChatOpenAI     from langchain.chains import LLMChain     from langchain.smith import RunEvalConfig, run_on_dataset          # Chains may have memory. Passing in a constructor function lets the     # evaluation framework avoid cross-contamination between runs.     def construct_chain():         llm = ChatOpenAI(temperature=0)         chain = LLMChain.from_string(             llm,             "What's the answer to {your_input_key}"         )         return chain          # Load off-the-shelf evaluators via config or the EvaluatorType (string or enum)     evaluation_config = RunEvalConfig(         evaluators=[             "qa",  # "Correctness" against a reference answer             "embedding_distance",             RunEvalConfig.Criteria("helpfulness"),             RunEvalConfig.Criteria({                 "fifth-grader-score": "Do you have to be smarter than a fifth grader to answer this question?"             }),         ]     )          client = Client()     run_on_dataset(         client,         "<my_dataset_name>",         construct_chain,         evaluation=evaluation_config,     )     

You can also create custom evaluators by subclassing the [`StringEvaluator`](https://python.langchain.com/api_reference/langchain/evaluation/langchain.evaluation.schema.StringEvaluator.html#langchain.evaluation.schema.StringEvaluator "langchain.evaluation.schema.StringEvaluator") or LangSmithâ€™s RunEvaluator classes.               from typing import Optional     from langchain.evaluation import StringEvaluator          class MyStringEvaluator(StringEvaluator):              @property         def requires_input(self) -> bool:             return False              @property         def requires_reference(self) -> bool:             return True              @property         def evaluation_name(self) -> str:             return "exact_match"              def _evaluate_strings(self, prediction, reference=None, input=None, **kwargs) -> dict:             return {"score": prediction == reference}               evaluation_config = RunEvalConfig(         custom_evaluators = [MyStringEvaluator()],     )          run_on_dataset(         client,         "<my_dataset_name>",         construct_chain,         evaluation=evaluation_config,     )     

**Primary Functions**

  * [`arun_on_dataset`](https://python.langchain.com/api_reference/langchain/smith/langchain.smith.evaluation.runner_utils.arun_on_dataset.html#langchain.smith.evaluation.runner_utils.arun_on_dataset "langchain.smith.evaluation.runner_utils.arun_on_dataset"): Asynchronous function to evaluate a chain, agent, or other LangChain component over a dataset.

  * [`run_on_dataset`](https://python.langchain.com/api_reference/langchain/smith/langchain.smith.evaluation.runner_utils.run_on_dataset.html#langchain.smith.evaluation.runner_utils.run_on_dataset "langchain.smith.evaluation.runner_utils.run_on_dataset"): Function to evaluate a chain, agent, or other LangChain component over a dataset.

  * [`RunEvalConfig`](https://python.langchain.com/api_reference/langchain/smith/langchain.smith.evaluation.config.RunEvalConfig.html#langchain.smith.evaluation.config.RunEvalConfig "langchain.smith.evaluation.config.RunEvalConfig"): Class representing the configuration for running evaluation. You can select evaluators by [`EvaluatorType`](https://python.langchain.com/api_reference/langchain/evaluation/langchain.evaluation.schema.EvaluatorType.html#langchain.evaluation.schema.EvaluatorType "langchain.evaluation.schema.EvaluatorType") or config, or you can pass in custom\_evaluators

**Classes**

[`smith.evaluation.config.EvalConfig`](https://python.langchain.com/api_reference/langchain/smith/langchain.smith.evaluation.config.EvalConfig.html#langchain.smith.evaluation.config.EvalConfig "langchain.smith.evaluation.config.EvalConfig") | Configuration for a given run evaluator.   ---|---   [`smith.evaluation.config.RunEvalConfig`](https://python.langchain.com/api_reference/langchain/smith/langchain.smith.evaluation.config.RunEvalConfig.html#langchain.smith.evaluation.config.RunEvalConfig "langchain.smith.evaluation.config.RunEvalConfig") | Configuration for a run evaluation.   [`smith.evaluation.config.SingleKeyEvalConfig`](https://python.langchain.com/api_reference/langchain/smith/langchain.smith.evaluation.config.SingleKeyEvalConfig.html#langchain.smith.evaluation.config.SingleKeyEvalConfig "langchain.smith.evaluation.config.SingleKeyEvalConfig") | Configuration for a run evaluator that only requires a single key.   [`smith.evaluation.progress.ProgressBarCallback`](https://python.langchain.com/api_reference/langchain/smith/langchain.smith.evaluation.progress.ProgressBarCallback.html#langchain.smith.evaluation.progress.ProgressBarCallback "langchain.smith.evaluation.progress.ProgressBarCallback")\(total\) | A simple progress bar for the console.   [`smith.evaluation.runner_utils.ChatModelInput`](https://python.langchain.com/api_reference/langchain/smith/langchain.smith.evaluation.runner_utils.ChatModelInput.html#langchain.smith.evaluation.runner_utils.ChatModelInput "langchain.smith.evaluation.runner_utils.ChatModelInput") | Input for a chat model.   [`smith.evaluation.runner_utils.EvalError`](https://python.langchain.com/api_reference/langchain/smith/langchain.smith.evaluation.runner_utils.EvalError.html#langchain.smith.evaluation.runner_utils.EvalError "langchain.smith.evaluation.runner_utils.EvalError")\(...\) | Your architecture raised an error.   [`smith.evaluation.runner_utils.InputFormatError`](https://python.langchain.com/api_reference/langchain/smith/langchain.smith.evaluation.runner_utils.InputFormatError.html#langchain.smith.evaluation.runner_utils.InputFormatError "langchain.smith.evaluation.runner_utils.InputFormatError") | Raised when the input format is invalid.   [`smith.evaluation.runner_utils.TestResult`](https://python.langchain.com/api_reference/langchain/smith/langchain.smith.evaluation.runner_utils.TestResult.html#langchain.smith.evaluation.runner_utils.TestResult "langchain.smith.evaluation.runner_utils.TestResult") | A dictionary of the results of a single test run.   [`smith.evaluation.string_run_evaluator.ChainStringRunMapper`](https://python.langchain.com/api_reference/langchain/smith/langchain.smith.evaluation.string_run_evaluator.ChainStringRunMapper.html#langchain.smith.evaluation.string_run_evaluator.ChainStringRunMapper "langchain.smith.evaluation.string_run_evaluator.ChainStringRunMapper") | Extract items to evaluate from the run object from a chain.   [`smith.evaluation.string_run_evaluator.LLMStringRunMapper`](https://python.langchain.com/api_reference/langchain/smith/langchain.smith.evaluation.string_run_evaluator.LLMStringRunMapper.html#langchain.smith.evaluation.string_run_evaluator.LLMStringRunMapper "langchain.smith.evaluation.string_run_evaluator.LLMStringRunMapper") | Extract items to evaluate from the run object.   [`smith.evaluation.string_run_evaluator.StringExampleMapper`](https://python.langchain.com/api_reference/langchain/smith/langchain.smith.evaluation.string_run_evaluator.StringExampleMapper.html#langchain.smith.evaluation.string_run_evaluator.StringExampleMapper "langchain.smith.evaluation.string_run_evaluator.StringExampleMapper") | Map an example, or row in the dataset, to the inputs of an evaluation.   [`smith.evaluation.string_run_evaluator.StringRunEvaluatorChain`](https://python.langchain.com/api_reference/langchain/smith/langchain.smith.evaluation.string_run_evaluator.StringRunEvaluatorChain.html#langchain.smith.evaluation.string_run_evaluator.StringRunEvaluatorChain "langchain.smith.evaluation.string_run_evaluator.StringRunEvaluatorChain") | Evaluate Run and optional examples.   [`smith.evaluation.string_run_evaluator.StringRunMapper`](https://python.langchain.com/api_reference/langchain/smith/langchain.smith.evaluation.string_run_evaluator.StringRunMapper.html#langchain.smith.evaluation.string_run_evaluator.StringRunMapper "langchain.smith.evaluation.string_run_evaluator.StringRunMapper") | Extract items to evaluate from the run object.   [`smith.evaluation.string_run_evaluator.ToolStringRunMapper`](https://python.langchain.com/api_reference/langchain/smith/langchain.smith.evaluation.string_run_evaluator.ToolStringRunMapper.html#langchain.smith.evaluation.string_run_evaluator.ToolStringRunMapper "langchain.smith.evaluation.string_run_evaluator.ToolStringRunMapper") | Map an input to the tool.      **Functions**

[`smith.evaluation.name_generation.random_name`](https://python.langchain.com/api_reference/langchain/smith/langchain.smith.evaluation.name_generation.random_name.html#langchain.smith.evaluation.name_generation.random_name "langchain.smith.evaluation.name_generation.random_name")\(\) | Generate a random name.   ---|---   [`smith.evaluation.runner_utils.arun_on_dataset`](https://python.langchain.com/api_reference/langchain/smith/langchain.smith.evaluation.runner_utils.arun_on_dataset.html#langchain.smith.evaluation.runner_utils.arun_on_dataset "langchain.smith.evaluation.runner_utils.arun_on_dataset")\(...\) | Run the Chain or language model on a dataset and store traces to the specified project name.   [`smith.evaluation.runner_utils.run_on_dataset`](https://python.langchain.com/api_reference/langchain/smith/langchain.smith.evaluation.runner_utils.run_on_dataset.html#langchain.smith.evaluation.runner_utils.run_on_dataset "langchain.smith.evaluation.runner_utils.run_on_dataset")\(...\) | Run the Chain or language model on a dataset and store traces to the specified project name.