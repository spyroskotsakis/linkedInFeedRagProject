# langchain.memory.chat_memory â€” ðŸ¦œðŸ”— LangChain  documentation

**URL:** https://python.langchain.com/api_reference/_modules/langchain/memory/chat_memory.html
**Word Count:** 111
**Links Count:** 17
**Scraped:** 2025-07-21 08:25:40
**Status:** completed

---

# Source code for langchain.memory.chat\_memory               import warnings     from abc import ABC     from typing import Any, Optional          from langchain_core._api import deprecated     from langchain_core.chat_history import (         BaseChatMessageHistory,         InMemoryChatMessageHistory,     )     from langchain_core.memory import BaseMemory     from langchain_core.messages import AIMessage, HumanMessage     from pydantic import Field          from langchain.memory.utils import get_prompt_input_key                              [[docs]](https://python.langchain.com/api_reference/langchain/memory/langchain.memory.chat_memory.BaseChatMemory.html#langchain.memory.buffer.BaseChatMemory)     @deprecated(         since="0.3.1",         removal="1.0.0",         message=(             "Please see the migration guide at: "             "https://python.langchain.com/docs/versions/migrating_memory/"         ),     )     class BaseChatMemory(BaseMemory, ABC):         """Abstract base class for chat memory.              **ATTENTION** This abstraction was created prior to when chat models had             native tool calling capabilities.             It does **NOT** support native tool calling capabilities for chat models and             will fail SILENTLY if used with a chat model that has native tool calling.              DO NOT USE THIS ABSTRACTION FOR NEW CODE.         """              chat_memory: BaseChatMessageHistory = Field(             default_factory=InMemoryChatMessageHistory,         )         output_key: Optional[str] = None         input_key: Optional[str] = None         return_messages: bool = False              def _get_input_output(             self,             inputs: dict[str, Any],             outputs: dict[str, str],         ) -> tuple[str, str]:             if self.input_key is None:                 prompt_input_key = get_prompt_input_key(inputs, self.memory_variables)             else:                 prompt_input_key = self.input_key             if self.output_key is None:                 if len(outputs) == 1:                     output_key = next(iter(outputs.keys()))                 elif "output" in outputs:                     output_key = "output"                     warnings.warn(                         f"'{self.__class__.__name__}' got multiple output keys:"                         f" {outputs.keys()}. The default 'output' key is being used."                         f" If this is not desired, please manually set 'output_key'.",                         stacklevel=3,                     )                 else:                     msg = (                         f"Got multiple output keys: {outputs.keys()}, cannot "                         f"determine which to store in memory. Please set the "                         f"'output_key' explicitly."                     )                     raise ValueError(msg)             else:                 output_key = self.output_key             return inputs[prompt_input_key], outputs[output_key]                         [[docs]](https://python.langchain.com/api_reference/langchain/memory/langchain.memory.chat_memory.BaseChatMemory.html#langchain.memory.buffer.BaseChatMemory.save_context)         def save_context(self, inputs: dict[str, Any], outputs: dict[str, str]) -> None:             """Save context from this conversation to buffer."""             input_str, output_str = self._get_input_output(inputs, outputs)             self.chat_memory.add_messages(                 [                     HumanMessage(content=input_str),                     AIMessage(content=output_str),                 ],             )                                        [[docs]](https://python.langchain.com/api_reference/langchain/memory/langchain.memory.chat_memory.BaseChatMemory.html#langchain.memory.buffer.BaseChatMemory.asave_context)         async def asave_context(             self,             inputs: dict[str, Any],             outputs: dict[str, str],         ) -> None:             """Save context from this conversation to buffer."""             input_str, output_str = self._get_input_output(inputs, outputs)             await self.chat_memory.aadd_messages(                 [                     HumanMessage(content=input_str),                     AIMessage(content=output_str),                 ],             )                                        [[docs]](https://python.langchain.com/api_reference/langchain/memory/langchain.memory.chat_memory.BaseChatMemory.html#langchain.memory.buffer.BaseChatMemory.clear)         def clear(self) -> None:             """Clear memory contents."""             self.chat_memory.clear()                                        [[docs]](https://python.langchain.com/api_reference/langchain/memory/langchain.memory.chat_memory.BaseChatMemory.html#langchain.memory.buffer.BaseChatMemory.aclear)         async def aclear(self) -> None:             """Clear memory contents."""             await self.chat_memory.aclear()