# evaluation â€” ðŸ¦œðŸ”— LangChain  documentation

**URL:** https://python.langchain.com/api_reference/langchain/evaluation.html
**Word Count:** 520
**Links Count:** 177
**Scraped:** 2025-07-21 07:51:39
**Status:** completed

---

# `evaluation`\#

**Evaluation** chains for grading LLM and Chain outputs.

This module contains off-the-shelf evaluation chains for grading the output of LangChain primitives such as language models and chains.

**Loading an evaluator**

To load an evaluator, you can use the [`load_evaluators`](https://python.langchain.com/api_reference/langchain/evaluation/langchain.evaluation.loading.load_evaluators.html#langchain.evaluation.loading.load_evaluators "langchain.evaluation.loading.load_evaluators") or [`load_evaluator`](https://python.langchain.com/api_reference/langchain/evaluation/langchain.evaluation.loading.load_evaluator.html#langchain.evaluation.loading.load_evaluator "langchain.evaluation.loading.load_evaluator") functions with the names of the evaluators to load.               from langchain.evaluation import load_evaluator          evaluator = load_evaluator("qa")     evaluator.evaluate_strings(         prediction="We sold more than 40,000 units last week",         input="How many units did we sell last week?",         reference="We sold 32,378 units",     )     

The evaluator must be one of [`EvaluatorType`](https://python.langchain.com/api_reference/langchain/evaluation/langchain.evaluation.schema.EvaluatorType.html#langchain.evaluation.schema.EvaluatorType "langchain.evaluation.schema.EvaluatorType").

**Datasets**

To load one of the LangChain HuggingFace datasets, you can use the [`load_dataset`](https://python.langchain.com/api_reference/langchain/evaluation/langchain.evaluation.loading.load_dataset.html#langchain.evaluation.loading.load_dataset "langchain.evaluation.loading.load_dataset") function with the name of the dataset to load.               from langchain.evaluation import load_dataset     ds = load_dataset("llm-math")     

**Some common use cases for evaluation include:**

  * Grading the accuracy of a response against ground truth answers: [`QAEvalChain`](https://python.langchain.com/api_reference/langchain/evaluation/langchain.evaluation.qa.eval_chain.QAEvalChain.html#langchain.evaluation.qa.eval_chain.QAEvalChain "langchain.evaluation.qa.eval_chain.QAEvalChain")

  * Comparing the output of two models: [`PairwiseStringEvalChain`](https://python.langchain.com/api_reference/langchain/evaluation/langchain.evaluation.comparison.eval_chain.PairwiseStringEvalChain.html#langchain.evaluation.comparison.eval_chain.PairwiseStringEvalChain "langchain.evaluation.comparison.eval_chain.PairwiseStringEvalChain") or [`LabeledPairwiseStringEvalChain`](https://python.langchain.com/api_reference/langchain/evaluation/langchain.evaluation.comparison.eval_chain.LabeledPairwiseStringEvalChain.html#langchain.evaluation.comparison.eval_chain.LabeledPairwiseStringEvalChain "langchain.evaluation.comparison.eval_chain.LabeledPairwiseStringEvalChain") when there is additionally a reference label.

  * Judging the efficacy of an agentâ€™s tool usage: [`TrajectoryEvalChain`](https://python.langchain.com/api_reference/langchain/evaluation/langchain.evaluation.agents.trajectory_eval_chain.TrajectoryEvalChain.html#langchain.evaluation.agents.trajectory_eval_chain.TrajectoryEvalChain "langchain.evaluation.agents.trajectory_eval_chain.TrajectoryEvalChain")

  * Checking whether an output complies with a set of criteria: [`CriteriaEvalChain`](https://python.langchain.com/api_reference/langchain/evaluation/langchain.evaluation.criteria.eval_chain.CriteriaEvalChain.html#langchain.evaluation.criteria.eval_chain.CriteriaEvalChain "langchain.evaluation.criteria.eval_chain.CriteriaEvalChain") or [`LabeledCriteriaEvalChain`](https://python.langchain.com/api_reference/langchain/evaluation/langchain.evaluation.criteria.eval_chain.LabeledCriteriaEvalChain.html#langchain.evaluation.criteria.eval_chain.LabeledCriteriaEvalChain "langchain.evaluation.criteria.eval_chain.LabeledCriteriaEvalChain") when there is additionally a reference label.

  * Computing semantic difference between a prediction and reference: [`EmbeddingDistanceEvalChain`](https://python.langchain.com/api_reference/langchain/evaluation/langchain.evaluation.embedding_distance.base.EmbeddingDistanceEvalChain.html#langchain.evaluation.embedding_distance.base.EmbeddingDistanceEvalChain "langchain.evaluation.embedding_distance.base.EmbeddingDistanceEvalChain") or between two predictions: [`PairwiseEmbeddingDistanceEvalChain`](https://python.langchain.com/api_reference/langchain/evaluation/langchain.evaluation.embedding_distance.base.PairwiseEmbeddingDistanceEvalChain.html#langchain.evaluation.embedding_distance.base.PairwiseEmbeddingDistanceEvalChain "langchain.evaluation.embedding_distance.base.PairwiseEmbeddingDistanceEvalChain")

  * Measuring the string distance between a prediction and reference [`StringDistanceEvalChain`](https://python.langchain.com/api_reference/langchain/evaluation/langchain.evaluation.string_distance.base.StringDistanceEvalChain.html#langchain.evaluation.string_distance.base.StringDistanceEvalChain "langchain.evaluation.string_distance.base.StringDistanceEvalChain") or between two predictions [`PairwiseStringDistanceEvalChain`](https://python.langchain.com/api_reference/langchain/evaluation/langchain.evaluation.string_distance.base.PairwiseStringDistanceEvalChain.html#langchain.evaluation.string_distance.base.PairwiseStringDistanceEvalChain "langchain.evaluation.string_distance.base.PairwiseStringDistanceEvalChain")

**Low-level API**

These evaluators implement one of the following interfaces:

  * [`StringEvaluator`](https://python.langchain.com/api_reference/langchain/evaluation/langchain.evaluation.schema.StringEvaluator.html#langchain.evaluation.schema.StringEvaluator "langchain.evaluation.schema.StringEvaluator"): Evaluate a prediction string against a reference label and/or input context.

  * [`PairwiseStringEvaluator`](https://python.langchain.com/api_reference/langchain/evaluation/langchain.evaluation.schema.PairwiseStringEvaluator.html#langchain.evaluation.schema.PairwiseStringEvaluator "langchain.evaluation.schema.PairwiseStringEvaluator"): Evaluate two prediction strings against each other. Useful for scoring preferences, measuring similarity between two chain or llm agents, or comparing outputs on similar inputs.

  * [`AgentTrajectoryEvaluator`](https://python.langchain.com/api_reference/langchain/evaluation/langchain.evaluation.schema.AgentTrajectoryEvaluator.html#langchain.evaluation.schema.AgentTrajectoryEvaluator "langchain.evaluation.schema.AgentTrajectoryEvaluator") Evaluate the full sequence of actions taken by an agent.

These interfaces enable easier composability and usage within a higher level evaluation framework.

**Classes**

[`evaluation.agents.trajectory_eval_chain.TrajectoryEval`](https://python.langchain.com/api_reference/langchain/evaluation/langchain.evaluation.agents.trajectory_eval_chain.TrajectoryEval.html#langchain.evaluation.agents.trajectory_eval_chain.TrajectoryEval "langchain.evaluation.agents.trajectory_eval_chain.TrajectoryEval") | A named tuple containing the score and reasoning for a trajectory.   ---|---   [`evaluation.agents.trajectory_eval_chain.TrajectoryEvalChain`](https://python.langchain.com/api_reference/langchain/evaluation/langchain.evaluation.agents.trajectory_eval_chain.TrajectoryEvalChain.html#langchain.evaluation.agents.trajectory_eval_chain.TrajectoryEvalChain "langchain.evaluation.agents.trajectory_eval_chain.TrajectoryEvalChain") | A chain for evaluating ReAct style agents.   [`evaluation.agents.trajectory_eval_chain.TrajectoryOutputParser`](https://python.langchain.com/api_reference/langchain/evaluation/langchain.evaluation.agents.trajectory_eval_chain.TrajectoryOutputParser.html#langchain.evaluation.agents.trajectory_eval_chain.TrajectoryOutputParser "langchain.evaluation.agents.trajectory_eval_chain.TrajectoryOutputParser") | Trajectory output parser.   [`evaluation.comparison.eval_chain.LabeledPairwiseStringEvalChain`](https://python.langchain.com/api_reference/langchain/evaluation/langchain.evaluation.comparison.eval_chain.LabeledPairwiseStringEvalChain.html#langchain.evaluation.comparison.eval_chain.LabeledPairwiseStringEvalChain "langchain.evaluation.comparison.eval_chain.LabeledPairwiseStringEvalChain") | A chain for comparing two outputs, such as the outputs   [`evaluation.comparison.eval_chain.PairwiseStringEvalChain`](https://python.langchain.com/api_reference/langchain/evaluation/langchain.evaluation.comparison.eval_chain.PairwiseStringEvalChain.html#langchain.evaluation.comparison.eval_chain.PairwiseStringEvalChain "langchain.evaluation.comparison.eval_chain.PairwiseStringEvalChain") | A chain for comparing two outputs, such as the outputs   [`evaluation.comparison.eval_chain.PairwiseStringResultOutputParser`](https://python.langchain.com/api_reference/langchain/evaluation/langchain.evaluation.comparison.eval_chain.PairwiseStringResultOutputParser.html#langchain.evaluation.comparison.eval_chain.PairwiseStringResultOutputParser "langchain.evaluation.comparison.eval_chain.PairwiseStringResultOutputParser") | A parser for the output of the PairwiseStringEvalChain.   [`evaluation.criteria.eval_chain.Criteria`](https://python.langchain.com/api_reference/langchain/evaluation/langchain.evaluation.criteria.eval_chain.Criteria.html#langchain.evaluation.criteria.eval_chain.Criteria "langchain.evaluation.criteria.eval_chain.Criteria")\(value\) | A Criteria to evaluate.   [`evaluation.criteria.eval_chain.CriteriaEvalChain`](https://python.langchain.com/api_reference/langchain/evaluation/langchain.evaluation.criteria.eval_chain.CriteriaEvalChain.html#langchain.evaluation.criteria.eval_chain.CriteriaEvalChain "langchain.evaluation.criteria.eval_chain.CriteriaEvalChain") | LLM Chain for evaluating runs against criteria.   [`evaluation.criteria.eval_chain.CriteriaResultOutputParser`](https://python.langchain.com/api_reference/langchain/evaluation/langchain.evaluation.criteria.eval_chain.CriteriaResultOutputParser.html#langchain.evaluation.criteria.eval_chain.CriteriaResultOutputParser "langchain.evaluation.criteria.eval_chain.CriteriaResultOutputParser") | A parser for the output of the CriteriaEvalChain.   [`evaluation.criteria.eval_chain.LabeledCriteriaEvalChain`](https://python.langchain.com/api_reference/langchain/evaluation/langchain.evaluation.criteria.eval_chain.LabeledCriteriaEvalChain.html#langchain.evaluation.criteria.eval_chain.LabeledCriteriaEvalChain "langchain.evaluation.criteria.eval_chain.LabeledCriteriaEvalChain") | Criteria evaluation chain that requires references.   [`evaluation.embedding_distance.base.EmbeddingDistance`](https://python.langchain.com/api_reference/langchain/evaluation/langchain.evaluation.embedding_distance.base.EmbeddingDistance.html#langchain.evaluation.embedding_distance.base.EmbeddingDistance "langchain.evaluation.embedding_distance.base.EmbeddingDistance")\(value\) | Embedding Distance Metric.   [`evaluation.embedding_distance.base.EmbeddingDistanceEvalChain`](https://python.langchain.com/api_reference/langchain/evaluation/langchain.evaluation.embedding_distance.base.EmbeddingDistanceEvalChain.html#langchain.evaluation.embedding_distance.base.EmbeddingDistanceEvalChain "langchain.evaluation.embedding_distance.base.EmbeddingDistanceEvalChain") | Use embedding distances to score semantic difference between a prediction and reference.   [`evaluation.embedding_distance.base.PairwiseEmbeddingDistanceEvalChain`](https://python.langchain.com/api_reference/langchain/evaluation/langchain.evaluation.embedding_distance.base.PairwiseEmbeddingDistanceEvalChain.html#langchain.evaluation.embedding_distance.base.PairwiseEmbeddingDistanceEvalChain "langchain.evaluation.embedding_distance.base.PairwiseEmbeddingDistanceEvalChain") | Use embedding distances to score semantic difference between two predictions.   [`evaluation.exact_match.base.ExactMatchStringEvaluator`](https://python.langchain.com/api_reference/langchain/evaluation/langchain.evaluation.exact_match.base.ExactMatchStringEvaluator.html#langchain.evaluation.exact_match.base.ExactMatchStringEvaluator "langchain.evaluation.exact_match.base.ExactMatchStringEvaluator")\(\*\) | Compute an exact match between the prediction and the reference.   [`evaluation.parsing.base.JsonEqualityEvaluator`](https://python.langchain.com/api_reference/langchain/evaluation/langchain.evaluation.parsing.base.JsonEqualityEvaluator.html#langchain.evaluation.parsing.base.JsonEqualityEvaluator "langchain.evaluation.parsing.base.JsonEqualityEvaluator")\(\[...\]\) | Evaluate whether the prediction is equal to the reference after   [`evaluation.parsing.base.JsonValidityEvaluator`](https://python.langchain.com/api_reference/langchain/evaluation/langchain.evaluation.parsing.base.JsonValidityEvaluator.html#langchain.evaluation.parsing.base.JsonValidityEvaluator "langchain.evaluation.parsing.base.JsonValidityEvaluator")\(...\) | Evaluate whether the prediction is valid JSON.   [`evaluation.parsing.json_distance.JsonEditDistanceEvaluator`](https://python.langchain.com/api_reference/langchain/evaluation/langchain.evaluation.parsing.json_distance.JsonEditDistanceEvaluator.html#langchain.evaluation.parsing.json_distance.JsonEditDistanceEvaluator "langchain.evaluation.parsing.json_distance.JsonEditDistanceEvaluator")\(\[...\]\) | An evaluator that calculates the edit distance between JSON strings.   [`evaluation.parsing.json_schema.JsonSchemaEvaluator`](https://python.langchain.com/api_reference/langchain/evaluation/langchain.evaluation.parsing.json_schema.JsonSchemaEvaluator.html#langchain.evaluation.parsing.json_schema.JsonSchemaEvaluator "langchain.evaluation.parsing.json_schema.JsonSchemaEvaluator")\(...\) | An evaluator that validates a JSON prediction against a JSON schema reference.   [`evaluation.qa.eval_chain.ContextQAEvalChain`](https://python.langchain.com/api_reference/langchain/evaluation/langchain.evaluation.qa.eval_chain.ContextQAEvalChain.html#langchain.evaluation.qa.eval_chain.ContextQAEvalChain "langchain.evaluation.qa.eval_chain.ContextQAEvalChain") | LLM Chain for evaluating QA w/o GT based on context   [`evaluation.qa.eval_chain.CotQAEvalChain`](https://python.langchain.com/api_reference/langchain/evaluation/langchain.evaluation.qa.eval_chain.CotQAEvalChain.html#langchain.evaluation.qa.eval_chain.CotQAEvalChain "langchain.evaluation.qa.eval_chain.CotQAEvalChain") | LLM Chain for evaluating QA using chain of thought reasoning.   [`evaluation.qa.eval_chain.QAEvalChain`](https://python.langchain.com/api_reference/langchain/evaluation/langchain.evaluation.qa.eval_chain.QAEvalChain.html#langchain.evaluation.qa.eval_chain.QAEvalChain "langchain.evaluation.qa.eval_chain.QAEvalChain") | LLM Chain for evaluating question answering.   [`evaluation.qa.generate_chain.QAGenerateChain`](https://python.langchain.com/api_reference/langchain/evaluation/langchain.evaluation.qa.generate_chain.QAGenerateChain.html#langchain.evaluation.qa.generate_chain.QAGenerateChain "langchain.evaluation.qa.generate_chain.QAGenerateChain") | LLM Chain for generating examples for question answering.   [`evaluation.regex_match.base.RegexMatchStringEvaluator`](https://python.langchain.com/api_reference/langchain/evaluation/langchain.evaluation.regex_match.base.RegexMatchStringEvaluator.html#langchain.evaluation.regex_match.base.RegexMatchStringEvaluator "langchain.evaluation.regex_match.base.RegexMatchStringEvaluator")\(\*\) | Compute a regex match between the prediction and the reference.   [`evaluation.schema.AgentTrajectoryEvaluator`](https://python.langchain.com/api_reference/langchain/evaluation/langchain.evaluation.schema.AgentTrajectoryEvaluator.html#langchain.evaluation.schema.AgentTrajectoryEvaluator "langchain.evaluation.schema.AgentTrajectoryEvaluator")\(\) | Interface for evaluating agent trajectories.   [`evaluation.schema.EvaluatorType`](https://python.langchain.com/api_reference/langchain/evaluation/langchain.evaluation.schema.EvaluatorType.html#langchain.evaluation.schema.EvaluatorType "langchain.evaluation.schema.EvaluatorType")\(value\) | The types of the evaluators.   [`evaluation.schema.LLMEvalChain`](https://python.langchain.com/api_reference/langchain/evaluation/langchain.evaluation.schema.LLMEvalChain.html#langchain.evaluation.schema.LLMEvalChain "langchain.evaluation.schema.LLMEvalChain") | A base class for evaluators that use an LLM.   [`evaluation.schema.PairwiseStringEvaluator`](https://python.langchain.com/api_reference/langchain/evaluation/langchain.evaluation.schema.PairwiseStringEvaluator.html#langchain.evaluation.schema.PairwiseStringEvaluator "langchain.evaluation.schema.PairwiseStringEvaluator")\(\) | Compare the output of two models \(or two outputs of the same model\).   [`evaluation.schema.StringEvaluator`](https://python.langchain.com/api_reference/langchain/evaluation/langchain.evaluation.schema.StringEvaluator.html#langchain.evaluation.schema.StringEvaluator "langchain.evaluation.schema.StringEvaluator")\(\) | Grade, tag, or otherwise evaluate predictions relative to their inputs and/or reference labels.   [`evaluation.scoring.eval_chain.LabeledScoreStringEvalChain`](https://python.langchain.com/api_reference/langchain/evaluation/langchain.evaluation.scoring.eval_chain.LabeledScoreStringEvalChain.html#langchain.evaluation.scoring.eval_chain.LabeledScoreStringEvalChain "langchain.evaluation.scoring.eval_chain.LabeledScoreStringEvalChain") | A chain for scoring the output of a model on a scale of 1-10.   [`evaluation.scoring.eval_chain.ScoreStringEvalChain`](https://python.langchain.com/api_reference/langchain/evaluation/langchain.evaluation.scoring.eval_chain.ScoreStringEvalChain.html#langchain.evaluation.scoring.eval_chain.ScoreStringEvalChain "langchain.evaluation.scoring.eval_chain.ScoreStringEvalChain") | A chain for scoring on a scale of 1-10 the output of a model.   [`evaluation.scoring.eval_chain.ScoreStringResultOutputParser`](https://python.langchain.com/api_reference/langchain/evaluation/langchain.evaluation.scoring.eval_chain.ScoreStringResultOutputParser.html#langchain.evaluation.scoring.eval_chain.ScoreStringResultOutputParser "langchain.evaluation.scoring.eval_chain.ScoreStringResultOutputParser") | A parser for the output of the ScoreStringEvalChain.   [`evaluation.string_distance.base.PairwiseStringDistanceEvalChain`](https://python.langchain.com/api_reference/langchain/evaluation/langchain.evaluation.string_distance.base.PairwiseStringDistanceEvalChain.html#langchain.evaluation.string_distance.base.PairwiseStringDistanceEvalChain "langchain.evaluation.string_distance.base.PairwiseStringDistanceEvalChain") | Compute string edit distances between two predictions.   [`evaluation.string_distance.base.StringDistance`](https://python.langchain.com/api_reference/langchain/evaluation/langchain.evaluation.string_distance.base.StringDistance.html#langchain.evaluation.string_distance.base.StringDistance "langchain.evaluation.string_distance.base.StringDistance")\(value\) | Distance metric to use.   [`evaluation.string_distance.base.StringDistanceEvalChain`](https://python.langchain.com/api_reference/langchain/evaluation/langchain.evaluation.string_distance.base.StringDistanceEvalChain.html#langchain.evaluation.string_distance.base.StringDistanceEvalChain "langchain.evaluation.string_distance.base.StringDistanceEvalChain") | Compute string distances between the prediction and the reference.      **Functions**

[`evaluation.comparison.eval_chain.resolve_pairwise_criteria`](https://python.langchain.com/api_reference/langchain/evaluation/langchain.evaluation.comparison.eval_chain.resolve_pairwise_criteria.html#langchain.evaluation.comparison.eval_chain.resolve_pairwise_criteria "langchain.evaluation.comparison.eval_chain.resolve_pairwise_criteria")\(...\) | Resolve the criteria for the pairwise evaluator.   ---|---   [`evaluation.criteria.eval_chain.resolve_criteria`](https://python.langchain.com/api_reference/langchain/evaluation/langchain.evaluation.criteria.eval_chain.resolve_criteria.html#langchain.evaluation.criteria.eval_chain.resolve_criteria "langchain.evaluation.criteria.eval_chain.resolve_criteria")\(...\) | Resolve the criteria to evaluate.   [`evaluation.loading.load_dataset`](https://python.langchain.com/api_reference/langchain/evaluation/langchain.evaluation.loading.load_dataset.html#langchain.evaluation.loading.load_dataset "langchain.evaluation.loading.load_dataset")\(uri\) | Load a dataset from the [LangChainDatasets on HuggingFace](https://huggingface.co/LangChainDatasets).   [`evaluation.loading.load_evaluator`](https://python.langchain.com/api_reference/langchain/evaluation/langchain.evaluation.loading.load_evaluator.html#langchain.evaluation.loading.load_evaluator "langchain.evaluation.loading.load_evaluator")\(evaluator, \*\) | Load the requested evaluation chain specified by a string.   [`evaluation.loading.load_evaluators`](https://python.langchain.com/api_reference/langchain/evaluation/langchain.evaluation.loading.load_evaluators.html#langchain.evaluation.loading.load_evaluators "langchain.evaluation.loading.load_evaluators")\(evaluators, \*\) | Load evaluators specified by a list of evaluator types.   [`evaluation.scoring.eval_chain.resolve_criteria`](https://python.langchain.com/api_reference/langchain/evaluation/langchain.evaluation.scoring.eval_chain.resolve_criteria.html#langchain.evaluation.scoring.eval_chain.resolve_criteria "langchain.evaluation.scoring.eval_chain.resolve_criteria")\(...\) | Resolve the criteria for the pairwise evaluator.