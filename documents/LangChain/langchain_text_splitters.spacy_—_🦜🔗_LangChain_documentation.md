# langchain_text_splitters.spacy â€” ðŸ¦œðŸ”— LangChain  documentation

**URL:** https://python.langchain.com/api_reference/_modules/langchain_text_splitters/spacy.html
**Word Count:** 69
**Links Count:** 15
**Scraped:** 2025-07-21 08:58:20
**Status:** completed

---

# Source code for langchain\_text\_splitters.spacy               from __future__ import annotations          from typing import Any          from langchain_text_splitters.base import TextSplitter                              [[docs]](https://python.langchain.com/api_reference/text_splitters/spacy/langchain_text_splitters.spacy.SpacyTextSplitter.html#langchain_text_splitters.spacy.SpacyTextSplitter)     class SpacyTextSplitter(TextSplitter):         """Splitting text using Spacy package.              Per default, Spacy's `en_core_web_sm` model is used and         its default max_length is 1000000 (it is the length of maximum character         this model takes which can be increased for large files). For a faster, but         potentially less accurate splitting, you can use `pipeline='sentencizer'`.         """                         [[docs]](https://python.langchain.com/api_reference/text_splitters/spacy/langchain_text_splitters.spacy.SpacyTextSplitter.html#langchain_text_splitters.spacy.SpacyTextSplitter.__init__)         def __init__(             self,             separator: str = "\n\n",             pipeline: str = "en_core_web_sm",             max_length: int = 1_000_000,             *,             strip_whitespace: bool = True,             **kwargs: Any,         ) -> None:             """Initialize the spacy text splitter."""             super().__init__(**kwargs)             self._tokenizer = _make_spacy_pipeline_for_splitting(                 pipeline, max_length=max_length             )             self._separator = separator             self._strip_whitespace = strip_whitespace                                        [[docs]](https://python.langchain.com/api_reference/text_splitters/spacy/langchain_text_splitters.spacy.SpacyTextSplitter.html#langchain_text_splitters.spacy.SpacyTextSplitter.split_text)         def split_text(self, text: str) -> list[str]:             """Split incoming text and return chunks."""             splits = (                 s.text if self._strip_whitespace else s.text_with_ws                 for s in self._tokenizer(text).sents             )             return self._merge_splits(splits, self._separator)                                             def _make_spacy_pipeline_for_splitting(         pipeline: str, *, max_length: int = 1_000_000     ) -> Any:  # avoid importing spacy         try:             import spacy         except ImportError:             msg = "Spacy is not installed, please install it with `pip install spacy`."             raise ImportError(msg)         if pipeline == "sentencizer":             sentencizer: Any = spacy.lang.en.English()             sentencizer.add_pipe("sentencizer")         else:             sentencizer = spacy.load(pipeline, exclude=["ner", "tagger"])             sentencizer.max_length = max_length         return sentencizer