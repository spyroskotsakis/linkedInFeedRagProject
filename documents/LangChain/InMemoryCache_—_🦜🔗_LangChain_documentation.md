# InMemoryCache â€” ðŸ¦œðŸ”— LangChain  documentation

**URL:** https://python.langchain.com/api_reference/community/cache/langchain_community.cache.InMemoryCache.html
**Word Count:** 102
**Links Count:** 156
**Scraped:** 2025-07-21 08:04:25
**Status:** completed

---

# InMemoryCache\#

_class _langchain\_community.cache.InMemoryCache[\[source\]](https://python.langchain.com/api_reference/_modules/langchain_community/cache.html#InMemoryCache)\#     

Cache that stores things in memory.

Initialize with empty cache.

Methods

`__init__`\(\) | Initialize with empty cache.   ---|---   `aclear`\(\*\*kwargs\) | Clear cache.   `alookup`\(prompt, llm\_string\) | Look up based on prompt and llm\_string.   `aupdate`\(prompt, llm\_string, return\_val\) | Update cache based on prompt and llm\_string.   `clear`\(\*\*kwargs\) | Clear cache.   `lookup`\(prompt, llm\_string\) | Look up based on prompt and llm\_string.   `update`\(prompt, llm\_string, return\_val\) | Update cache based on prompt and llm\_string.      \_\_init\_\_\(\) â†’ None[\[source\]](https://python.langchain.com/api_reference/_modules/langchain_community/cache.html#InMemoryCache.__init__)\#     

Initialize with empty cache.

Return type:     

None

_async _aclear\(_\*\* kwargs: Any_\) â†’ None[\[source\]](https://python.langchain.com/api_reference/_modules/langchain_community/cache.html#InMemoryCache.aclear)\#     

Clear cache.

Parameters:     

**kwargs** \(_Any_\)

Return type:     

None

_async _alookup\(

    _prompt : str_,     _llm\_string : str_, \) â†’ Sequence\[[Generation](https://python.langchain.com/api_reference/core/outputs/langchain_core.outputs.generation.Generation.html#langchain_core.outputs.generation.Generation "langchain_core.outputs.generation.Generation")\] | None[\[source\]](https://python.langchain.com/api_reference/_modules/langchain_community/cache.html#InMemoryCache.alookup)\#     

Look up based on prompt and llm\_string.

Parameters:     

  * **prompt** \(_str_\)

  * **llm\_string** \(_str_\)

Return type:     

_Sequence_\[[_Generation_](https://python.langchain.com/api_reference/core/outputs/langchain_core.outputs.generation.Generation.html#langchain_core.outputs.generation.Generation "langchain_core.outputs.generation.Generation")\] | None

_async _aupdate\(

    _prompt : str_,     _llm\_string : str_,     _return\_val : Sequence\[[Generation](https://python.langchain.com/api_reference/core/outputs/langchain_core.outputs.generation.Generation.html#langchain_core.outputs.generation.Generation "langchain_core.outputs.generation.Generation")\]_, \) â†’ None[\[source\]](https://python.langchain.com/api_reference/_modules/langchain_community/cache.html#InMemoryCache.aupdate)\#     

Update cache based on prompt and llm\_string.

Parameters:     

  * **prompt** \(_str_\)

  * **llm\_string** \(_str_\)

  * **return\_val** \(_Sequence_ _\[_[_Generation_](https://python.langchain.com/api_reference/core/outputs/langchain_core.outputs.generation.Generation.html#langchain_core.outputs.generation.Generation "langchain_core.outputs.generation.Generation") _\]_\)

Return type:     

None

clear\(_\*\* kwargs: Any_\) â†’ None[\[source\]](https://python.langchain.com/api_reference/_modules/langchain_community/cache.html#InMemoryCache.clear)\#     

Clear cache.

Parameters:     

**kwargs** \(_Any_\)

Return type:     

None

lookup\(

    _prompt : str_,     _llm\_string : str_, \) â†’ Sequence\[[Generation](https://python.langchain.com/api_reference/core/outputs/langchain_core.outputs.generation.Generation.html#langchain_core.outputs.generation.Generation "langchain_core.outputs.generation.Generation")\] | None[\[source\]](https://python.langchain.com/api_reference/_modules/langchain_community/cache.html#InMemoryCache.lookup)\#     

Look up based on prompt and llm\_string.

Parameters:     

  * **prompt** \(_str_\)

  * **llm\_string** \(_str_\)

Return type:     

_Sequence_\[[_Generation_](https://python.langchain.com/api_reference/core/outputs/langchain_core.outputs.generation.Generation.html#langchain_core.outputs.generation.Generation "langchain_core.outputs.generation.Generation")\] | None

update\(

    _prompt : str_,     _llm\_string : str_,     _return\_val : Sequence\[[Generation](https://python.langchain.com/api_reference/core/outputs/langchain_core.outputs.generation.Generation.html#langchain_core.outputs.generation.Generation "langchain_core.outputs.generation.Generation")\]_, \) â†’ None[\[source\]](https://python.langchain.com/api_reference/_modules/langchain_community/cache.html#InMemoryCache.update)\#     

Update cache based on prompt and llm\_string.

Parameters:     

  * **prompt** \(_str_\)

  * **llm\_string** \(_str_\)

  * **return\_val** \(_Sequence_ _\[_[_Generation_](https://python.langchain.com/api_reference/core/outputs/langchain_core.outputs.generation.Generation.html#langchain_core.outputs.generation.Generation "langchain_core.outputs.generation.Generation") _\]_\)

Return type:     

None

Examples using InMemoryCache

  * [How to cache LLM responses](https://python.langchain.com/docs/how_to/llm_caching/)

  * [How to cache chat model responses](https://python.langchain.com/docs/how_to/chat_model_caching/)

  * [Model caches](https://python.langchain.com/docs/integrations/llm_caching/)

__On this page