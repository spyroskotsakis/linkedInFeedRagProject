# ConversationVectorStoreTokenBufferMemory â€” ðŸ¦œðŸ”— LangChain  documentation

**URL:** https://python.langchain.com/api_reference/langchain/memory/langchain.memory.vectorstore_token_buffer_memory.ConversationVectorStoreTokenBufferMemory.html
**Word Count:** 276
**Links Count:** 158
**Scraped:** 2025-07-21 07:51:08
**Status:** completed

---

# ConversationVectorStoreTokenBufferMemory\#

_class _langchain.memory.vectorstore\_token\_buffer\_memory.ConversationVectorStoreTokenBufferMemory[\[source\]](https://python.langchain.com/api_reference/_modules/langchain/memory/vectorstore_token_buffer_memory.html#ConversationVectorStoreTokenBufferMemory)\#     

Bases: [`ConversationTokenBufferMemory`](https://python.langchain.com/api_reference/langchain/memory/langchain.memory.token_buffer.ConversationTokenBufferMemory.html#langchain.memory.token_buffer.ConversationTokenBufferMemory "langchain.memory.token_buffer.ConversationTokenBufferMemory")

Conversation chat memory with token limit and vectordb backing.

load\_memory\_variables\(\) will return a dict with the key â€œhistoryâ€. It contains background information retrieved from the vector store plus recent lines of the current conversation.

To help the LLM understand the part of the conversation stored in the vectorstore, each interaction is timestamped and the current date and time is also provided in the history. A side effect of this is that the LLM will have access to the current date and time.

Initialization arguments:

This class accepts all the initialization arguments of ConversationTokenBufferMemory, such as llm. In addition, it accepts the following additional arguments

> retriever: \(required\) A VectorStoreRetriever object to use >      >  > as the vector backing store >  > split\_chunk\_size: \(optional, 1000\) Token chunk split size >      >  > for long messages generated by the AI >  > previous\_history\_template: \(optional\) Template used to format >      >  > the contents of the prompt history

Example using ChromaDB:               from langchain.memory.token_buffer_vectorstore_memory import (             ConversationVectorStoreTokenBufferMemory     )     from langchain_chroma import Chroma     from langchain_community.embeddings import HuggingFaceInstructEmbeddings     from langchain_openai import OpenAI          embedder = HuggingFaceInstructEmbeddings(                     query_instruction="Represent the query for retrieval: "     )     chroma = Chroma(collection_name="demo",                     embedding_function=embedder,                     collection_metadata={"hnsw:space": "cosine"},                     )          retriever = chroma.as_retriever(             search_type="similarity_score_threshold",             search_kwargs={                 'k': 5,                 'score_threshold': 0.75,             },     )          conversation_memory = ConversationVectorStoreTokenBufferMemory(             return_messages=True,             llm=OpenAI(),             retriever=retriever,             max_token_limit = 1000,     )          conversation_memory.save_context({"Human": "Hi there"},                                       {"AI": "Nice to meet you!"}     )     conversation_memory.save_context({"Human": "Nice day isn't it?"},                                       {"AI": "I love Wednesdays."}     )     conversation_memory.load_memory_variables({"input": "What time is it?"})     

_param _ai\_prefix _: str_ _ = 'AI'_\#     

_param _chat\_memory _: [BaseChatMessageHistory](https://python.langchain.com/api_reference/core/chat_history/langchain_core.chat_history.BaseChatMessageHistory.html#langchain_core.chat_history.BaseChatMessageHistory "langchain_core.chat_history.BaseChatMessageHistory")_ _\[Optional\]_\#     

_param _human\_prefix _: str_ _ = 'Human'_\#     

_param _input\_key _: str | None_ _ = None_\#     

_param _llm _: [BaseLanguageModel](https://python.langchain.com/api_reference/core/language_models/langchain_core.language_models.base.BaseLanguageModel.html#langchain_core.language_models.base.BaseLanguageModel "langchain_core.language_models.base.BaseLanguageModel")_ _\[Required\]_\#     

_param _max\_token\_limit _: int_ _ = 2000_\#     

_param _memory\_key _: str_ _ = 'history'_\#     

_param _output\_key _: str | None_ _ = None_\#     

_param _previous\_history\_template _: str_ _ = '\nCurrent date and time: \{current\_time\}.\n\nPotentially relevant timestamped excerpts of previous conversations \(you\ndo not need to use these if irrelevant\):\n\{previous\_history\}\n\n'_\#     

_param _retriever _: [VectorStoreRetriever](https://python.langchain.com/api_reference/core/vectorstores/langchain_core.vectorstores.base.VectorStoreRetriever.html#langchain_core.vectorstores.base.VectorStoreRetriever "langchain_core.vectorstores.base.VectorStoreRetriever")_ _\[Required\]_\#     

_param _return\_messages _: bool_ _ = False_\#     

_param _split\_chunk\_size _: int_ _ = 1000_\#     

_async _aclear\(\) â†’ None\#     

Clear memory contents.

Return type:     

None

_async _aload\_memory\_variables\(

    _inputs : dict\[str, Any\]_, \) â†’ dict\[str, Any\]\#     

Async return key-value pairs given the text input to the chain.

Parameters:     

**inputs** \(_dict_ _\[__str_ _,__Any_ _\]_\) â€“ The inputs to the chain.

Returns:     

A dictionary of key-value pairs.

Return type:     

dict\[str, _Any_\]

_async _asave\_context\(

    _inputs : dict\[str, Any\]_,     _outputs : dict\[str, str\]_, \) â†’ None\#     

Save context from this conversation to buffer.

Parameters:     

  * **inputs** \(_dict_ _\[__str_ _,__Any_ _\]_\)

  * **outputs** \(_dict_ _\[__str_ _,__str_ _\]_\)

Return type:     

None

clear\(\) â†’ None\#     

Clear memory contents.

Return type:     

None

load\_memory\_variables\(

    _inputs : dict\[str, Any\]_, \) â†’ dict\[str, Any\][\[source\]](https://python.langchain.com/api_reference/_modules/langchain/memory/vectorstore_token_buffer_memory.html#ConversationVectorStoreTokenBufferMemory.load_memory_variables)\#     

Return history and memory buffer.

Parameters:     

**inputs** \(_dict_ _\[__str_ _,__Any_ _\]_\)

Return type:     

dict\[str, _Any_\]

save\_context\(

    _inputs : dict\[str, Any\]_,     _outputs : dict\[str, str\]_, \) â†’ None[\[source\]](https://python.langchain.com/api_reference/_modules/langchain/memory/vectorstore_token_buffer_memory.html#ConversationVectorStoreTokenBufferMemory.save_context)\#     

Save context from this conversation to buffer. Pruned.

Parameters:     

  * **inputs** \(_dict_ _\[__str_ _,__Any_ _\]_\)

  * **outputs** \(_dict_ _\[__str_ _,__str_ _\]_\)

Return type:     

None

save\_remainder\(\) â†’ None[\[source\]](https://python.langchain.com/api_reference/_modules/langchain/memory/vectorstore_token_buffer_memory.html#ConversationVectorStoreTokenBufferMemory.save_remainder)\#     

Save the remainder of the conversation buffer to the vector store.

This is useful if you have made the vectorstore persistent, in which case this can be called before the end of the session to store the remainder of the conversation.

Return type:     

None

_property _buffer _: Any_\#     

String buffer of memory.

_property _buffer\_as\_messages _: list\[[BaseMessage](https://python.langchain.com/api_reference/core/messages/langchain_core.messages.base.BaseMessage.html#langchain_core.messages.base.BaseMessage "langchain_core.messages.base.BaseMessage")\]_\#     

Exposes the buffer as a list of messages in case return\_messages is True.

_property _buffer\_as\_str _: str_\#     

Exposes the buffer as a string in case return\_messages is False.

_property _memory\_retriever _: [VectorStoreRetrieverMemory](https://python.langchain.com/api_reference/langchain/memory/langchain.memory.vectorstore.VectorStoreRetrieverMemory.html#langchain.memory.vectorstore.VectorStoreRetrieverMemory "langchain.memory.vectorstore.VectorStoreRetrieverMemory")_\#     

Return a memory retriever from the passed retriever object.

__On this page