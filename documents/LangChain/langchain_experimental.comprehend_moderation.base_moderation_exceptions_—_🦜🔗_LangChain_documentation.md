# langchain_experimental.comprehend_moderation.base_moderation_exceptions â€” ðŸ¦œðŸ”— LangChain  documentation

**URL:** https://python.langchain.com/api_reference/_modules/langchain_experimental/comprehend_moderation/base_moderation_exceptions.html
**Word Count:** 61
**Links Count:** 15
**Scraped:** 2025-07-21 09:19:27
**Status:** completed

---

# Source code for langchain\_experimental.comprehend\_moderation.base\_moderation\_exceptions                              [[docs]](https://python.langchain.com/api_reference/experimental/comprehend_moderation/langchain_experimental.comprehend_moderation.base_moderation_exceptions.ModerationPiiError.html#langchain_experimental.comprehend_moderation.base_moderation_exceptions.ModerationPiiError)     class ModerationPiiError(Exception):         """Exception raised if PII entities are detected.              Attributes:             message -- explanation of the error         """              def __init__(             self, message: str = "The prompt contains PII entities and cannot be processed"         ):             self.message = message             super().__init__(self.message)                                             [[docs]](https://python.langchain.com/api_reference/experimental/comprehend_moderation/langchain_experimental.comprehend_moderation.base_moderation_exceptions.ModerationToxicityError.html#langchain_experimental.comprehend_moderation.base_moderation_exceptions.ModerationToxicityError)     class ModerationToxicityError(Exception):         """Exception raised if Toxic entities are detected.              Attributes:             message -- explanation of the error         """              def __init__(             self, message: str = "The prompt contains toxic content and cannot be processed"         ):             self.message = message             super().__init__(self.message)                                             [[docs]](https://python.langchain.com/api_reference/experimental/comprehend_moderation/langchain_experimental.comprehend_moderation.base_moderation_exceptions.ModerationPromptSafetyError.html#langchain_experimental.comprehend_moderation.base_moderation_exceptions.ModerationPromptSafetyError)     class ModerationPromptSafetyError(Exception):         """Exception raised if Unsafe prompts are detected.              Attributes:             message -- explanation of the error         """              def __init__(             self,             message: str = ("The prompt is unsafe and cannot be processed"),         ):             self.message = message             super().__init__(self.message)